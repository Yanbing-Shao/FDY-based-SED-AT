experiment_name: '[8, 16, 24]_[1, 0.5,0]_32k_log+pcen_baseline_SONYC_strong_0921_comp_0919'
model_type: "CRNN" # para to determine which mode to be used (not test yet)
training:
  train_on_fine_taxo: True
  #batch size: [SGP, SONYC, SGP_unlab]
  batch_size: [8, 16, 24]
  batch_size_val: 128  # try a small val then drop last - roll back to 128
  #Â supervised loss weights for [SGP_strong, SONYC_weak, SGP_weak]
  weight_loss_sup: [1, 0.5, 0.1] #[0, 1, 0] for system totally trained on SONYC
  scheduler_type : "none"
  const_max: 2 # max weight used for self supervised loss
  n_steps_warmup: # num steps used for exponential warmup
  n_epochs_warmup: 30 # num epochs used for exponential warmup
  num_workers: 6 # change according to your cpu
  n_epochs: 220 # max num epochs
  early_stop_patience: 60
  accumulate_batches: 1
  gradient_clip: 0 # 0 no gradient clipping
  median_window: 7 # length of median filter used to smooth prediction in inference (nb of output frames)
  val_thresholds: [0.5] # thresholds used to compute f1 intersection in validation.
  n_test_thresholds: 50 # number of thresholds used to compute psds in test
  ema_factor: 0.999 # ema factor for mean teacher
  supervised_loss: focal
  self_sup_loss: mse # bce or mse for self supervised mean teacher loss
  backend: # pytorch lightning backend, ddp, dp or None
  limit_train_batches: 562 # 142 prev - for 4435 SGP audio with bs = 32
  validation_interval: 1 # perform validation every X epoch, 1 default
  check_train_every_n_epochs: 0 # Evaluate classification metrics on the train set every n epochs, default 10
  seed: 168
  mixup: soft # Soft mixup gives the ratio of the mix to the labels, hard mixup gives a 1 to every label present.
  obj_metric_synth_type: detection
  early_stopping: True
scaler:
  statistic: instance # instance or dataset-wide statistic
  normtype: standard # minmax or standard or mean normalization
  dims: [1,2] # dimensions over which normalization is applied - input feature shape (B, H, W)
  savepath: ./scaler.ckpt # path to scaler checkpoint

opt:
  name: adam
  lr: 0.0005
features:
  n_mels: 128
  n_filters: 2048
  hop_length: 512
  n_window: 2048
  sample_rate: 32000
  f_min: 0
  f_max: 16000
  transform_type: log+pcen
  pcen_trainable: True

transform:
  filter_db_range: [ -4.5, 6 ]       # db range of FilterAugment to be applied on each band
  filter_bands: [ 2, 5 ]             # range of frequency band number in FilterAugment
  filter_minimum_bandwidth: 4
  filter_type: step


net:
  dropout: 0.4
  rnn_layers: 2
  n_in_channel: 1
  nclass: 15
  attention: True
  n_RNN_cell: 128
  activation: glu
  rnn_type: BGRU
  kernel_size: [3, 3, 3, 3, 3, 3, 3]
  padding: [1, 1, 1, 1, 1, 1, 1]
  stride: [1, 1, 1, 1, 1, 1, 1]
  nb_filters: [ 16, 32, 64, 128, 128, 128, 128]
  pooling: [ [ 2, 2 ], [ 2, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ] ]
  dropout_recurrent: 0

data:
  audio_max_len: 10
  fs: 32000
  net_subsample: 4
  root_path: /home/tianzichen/SELDProjects/CoSMo/data/ # put the root path of the data folder here
  hdf5_train: "train.h5"
  hdf5_val: "val.h5"
  hdf5_test: "test.h5"
  sonyc_csv_train: "metadata/train/SONYC_train.csv"
  sonyc_csv_val: "metadata/val/SONYC_val.csv"
  sonyc_csv_test: "metadata/test/SONYC_test.csv"
  raw_singa-pura_csv_root: "/home/tianzichen/sound_datasets/singapura/labels_public"
  singa-pura_csv_train: "metadata/train/SINGA-PURA_train.csv"
  singa-pura_csv_val: "metadata/val/SINGA-PURA_val.csv"
  singa-pura_csv_test: "metadata/test/SINGA-PURA_test.csv"
  singa-pura_csv_unlabelled: "metadata/train/SINGA-PURA_unlabelled.csv"
  taxonomy_path: "/home/tianzichen/SELDProjects/CoSMo/config/taxonomy_SONYC.yaml" #it seems that there's an taxonomy_SONYC.yaml file in config folder

